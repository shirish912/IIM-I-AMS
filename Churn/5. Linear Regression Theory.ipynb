{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a8cc14",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) for Linear Regresssion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512832c",
   "metadata": {},
   "source": [
    "Ordinary Least Squares (OLS) is a method of estimating the unknown parameters in a linear regression model. The OLS method minimizes the sum of the squared differences between the observed values and the values predicted by the linear model. Here's a detailed explanation:\n",
    "\n",
    "### Concept:\n",
    "\n",
    "1. **Linear Model Representation**:\n",
    "   - A linear model aims to describe the relationship between a dependent variable $(Y)$ and one or more independent variables $ X_i $ by fitting a linear equation to the observed data.\n",
    "   - The general form of a linear regression model with $ n $ predictors is: \n",
    "     $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon $\n",
    "   - Here, $ Y $ is the dependent variable, $ X_i $ are the independent variables, $ \\beta_i $ are the coefficients to be estimated, and $ \\epsilon $ is the error term.\n",
    "\n",
    "2. **The Goal of OLS**:\n",
    "   - The primary goal of OLS regression is to find the best-fitting line through the data points. \"Best-fitting\" means that the sum of the squared differences (residuals) between the observed values and the values predicted by the model is minimized.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "1. **Residuals**:\n",
    "   - The residual for each observation is the difference between the observed value of the dependent variable and the value predicted by the model.\n",
    "   - Residual for the $ i $-th observation: $ e_i = y_i - (\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_n x_{in}) $\n",
    "\n",
    "2. **Objective Function (Sum of Squared Residuals)**:\n",
    "   - The OLS method seeks to minimize the sum of the squared residuals: \n",
    "     $\\text{Minimize} \\sum_{i=1}^{N} e_i^2 $\n",
    "   - This is known as the Least Squares criterion.\n",
    "\n",
    "### Estimation Process:\n",
    "\n",
    "1. **Estimating Coefficients**:\n",
    "   - Using calculus, the coefficients $ \\beta_i $ are estimated by finding the values that minimize the objective function.\n",
    "   - This typically involves taking the partial derivatives of the objective function with respect to each coefficient and setting them to zero to solve for the coefficients.\n",
    "\n",
    "2. **Matrix Notation**:\n",
    "   - In matrix terms, the coefficients are often estimated using the formula: \n",
    "     $\\hat{\\beta} = (X'X)^{-1}X'Y $\n",
    "   - Here, $ X $ is the matrix of input features, and $ Y $ is the vector of the output variable. $ \\hat{\\beta} $ is the vector of estimated coefficients.\n",
    "\n",
    "### Assumptions of OLS:\n",
    "\n",
    "The OLS method makes several key assumptions, including linearity, independence, homoscedasticity, and normality of errors, as discussed in previous messages. Violations of these assumptions can lead to biased or inefficient estimates.\n",
    "\n",
    "### Advantages and Disadvantages:\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simplicity and interpretability.\n",
    "  - Basis for many other regression methods and statistical analyses.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Susceptible to outliers.\n",
    "  - Requires adherence to its underlying assumptions for valid results.\n",
    "  - Can only model linear relationships (unless transformations are applied).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255a1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01c95eb8",
   "metadata": {},
   "source": [
    "# Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b3cfb",
   "metadata": {},
   "source": [
    "The fundamental assumptions of linear regression are critical for the validity of the model's results. These assumptions are:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear. This means the change in the dependent variable due to a one-unit change in any of the independent variables is constant.\n",
    "\n",
    "2. **Independence**: Observations are independent of each other. In other words, the values of the dependent variable for one observation are not influenced by the values of another observation.\n",
    "\n",
    "3. **Homoscedasticity**: The residuals (or errors) exhibit constant variance. This means that the spread of the residuals should be roughly the same across all levels of the independent variables. If the variance of the residuals changes significantly, it's known as heteroscedasticity.\n",
    "\n",
    "4. **Normal Distribution of Errors**: For any fixed value of the independent variables, the dependent variable is normally distributed. While the dependent variable itself doesn’t have to be normally distributed, the distribution of the errors (or residuals) should ideally follow a normal distribution for making inferences about the coefficients.\n",
    "\n",
    "5. **No or Little Multicollinearity**: Multicollinearity occurs when independent variables in a regression model are highly correlated. This correlation can make it difficult to determine the individual effect of each independent variable on the dependent variable. Ideally, the model should have little or no multicollinearity.\n",
    "\n",
    "6. **No Auto-correlation**: This assumption is particularly relevant in time series data. Auto-correlation occurs when the residuals are not independent from each other. For instance, in a time series scenario, this would mean that the error for one time period is correlated with the error for another time period.\n",
    "\n",
    "It's important to test these assumptions before fully interpreting the results of a linear regression analysis. Violations of these assumptions can lead to biased or inefficient estimates of the regression coefficients, ultimately affecting the predictive power and inferential validity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a7f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c68b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a527671",
   "metadata": {},
   "source": [
    "# R-squared and Adjusted R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cb7a7",
   "metadata": {},
   "source": [
    "R-squared and Adjusted R-squared are statistical measures used to assess the goodness of fit of a regression model. They provide insights into the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "### R-squared (Coefficient of Determination):\n",
    "\n",
    "1. **Definition**: R-squared is the proportion of the variance in the dependent variable that is predictable from the independent variables. It's a measure of how well the observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.\n",
    "\n",
    "2. **Calculation**: It is calculated as:\n",
    "   $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "   Where:\n",
    "   - $SS_{res}$ is the sum of squares of residuals (also known as the sum of squared errors of prediction).\n",
    "   - $SS_{tot}$ is the total sum of squares (proportional to the variance of the data).\n",
    "\n",
    "3. **Range**: R-squared ranges from 0 to 1. An R-squared of 0 means that the model explains none of the variability of the response data around its mean, while an R-squared of 1 means that it explains all the variability.\n",
    "\n",
    "4. **Interpretation**: \n",
    "   - A higher R-squared value indicates a higher proportion of variance in the dependent variable being explained by the independent variables.\n",
    "   - However, a high R-squared does not necessarily mean the model is good. It could be misleading, especially if the model is overfitted or if the regression has many predictors.\n",
    "\n",
    "### Adjusted R-squared:\n",
    "\n",
    "1. **Definition**: Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. It incorporates the model’s degrees of freedom.\n",
    "\n",
    "2. **Calculation**: It is calculated as:\n",
    "   $\\text{Adjusted } R^2 = 1 - \\left(1-R^2\\right)\\frac{n-1}{n-p-1}$\n",
    "   Where:\n",
    "   - $n$ is the number of observations.\n",
    "   - $p$ is the number of predictors.\n",
    "   - $R^2$ is the R-squared of the model.\n",
    "\n",
    "3. **Importance**: \n",
    "   - Unlike R-squared, Adjusted R-squared increases only if the new term improves the model more than would be expected by chance. \n",
    "   - It decreases when a predictor improves the model by less than expected by chance.\n",
    "   - It is more suitable for comparing models with a different number of predictors.\n",
    "\n",
    "4. **Interpretation**: \n",
    "   - A model with more terms may have a higher R-squared but not a higher Adjusted R-squared.\n",
    "   - Adjusted R-squared is generally a better measure for the goodness of fit for a regression model that includes multiple predictors.\n",
    "\n",
    "In summary, while R-squared gives you an estimate of the strength of the relationship between your model and the dependent variable, Adjusted R-squared also accounts for the number of predictors and will penalize you for adding predictors that do not improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086d55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
