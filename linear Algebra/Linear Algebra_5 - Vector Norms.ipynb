{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3543862",
   "metadata": {},
   "source": [
    "# Vector Norms\n",
    "\n",
    "In mathematics, particularly in linear algebra and functional analysis, a **vector norm** is a function that assigns a non-negative length or size to each vector in a vector space—except for the zero vector, which is assigned a length of zero. Essentially, norms measure the magnitude of vectors and are designed to satisfy specific properties that make them useful for various mathematical and computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dab75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "676c9ec3",
   "metadata": {},
   "source": [
    "The **p-norm** (also known as the **Lp norm**) of a vector provides a general way to define the length of the vector in $ \\mathbb{R}^n $. The p-norm is defined for a vector $ \\mathbf{x} = (x_1, x_2, ..., x_n) $ and a real number $ p \\geq 1 $ as follows:\n",
    "\n",
    "### Mathematical Formula for the p-norm\n",
    "\n",
    "The p-norm of the vector $ \\mathbf{x} $ is given by the formula:\n",
    "\n",
    "$$\n",
    "\\| \\mathbf{x} \\|_p = \\left( |x_1|^p + |x_2|^p + \\cdots + |x_n|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ |x_i|^p $ denotes the absolute value of $ x_i $ raised to the power of $ p $.\n",
    "- The expression inside the parentheses is the sum of these powered absolute values for all components of the vector.\n",
    "- The entire sum is then raised to the power of $ \\frac{1}{p} $.\n",
    "\n",
    "### Special Cases of the p-norm\n",
    "\n",
    "1. **Manhattan Norm (L1 Norm)**:\n",
    "   - When $ p = 1 $, the p-norm becomes the Manhattan norm (or L1 norm), which sums the absolute values of the components of the vector.\n",
    "   $$\n",
    "   \\| \\mathbf{x} \\|_1 = |x_1| + |x_2| + \\cdots + |x_n|\n",
    "   $$\n",
    "\n",
    "2. **Euclidean Norm (L2 Norm)**:\n",
    "   - When $ p = 2 $, the p-norm becomes the Euclidean norm (or L2 norm), which measures the \"usual\" straight-line distance from the origin to the point in n-dimensional space.\n",
    "   $$\n",
    "   \\| \\mathbf{x} \\|_2 = \\sqrt{|x_1|^2 + |x_2|^2 + \\cdots + |x_n|^2}\n",
    "   $$\n",
    "\n",
    "3. **Infinity Norm (Max Norm)**:\n",
    "   - As $ p $ approaches infinity, the p-norm converges to the infinity norm (or max norm), which is the maximum absolute value among the components of the vector.\n",
    "   $$\n",
    "   \\| \\mathbf{x} \\|_{\\infty} = \\max(|x_1|, |x_2|, ..., |x_n|)\n",
    "   $$\n",
    "\n",
    "### Properties of the p-norm\n",
    "\n",
    "The p-norms are designed to satisfy several mathematical properties essential for normed vector spaces:\n",
    "- **Non-negativity**: $ \\| \\mathbf{x} \\|_p \\geq 0 $ for all $ \\mathbf{x} $ and $ \\| \\mathbf{x} \\|_p = 0 $ if and only if $ \\mathbf{x} = 0 $.\n",
    "- **Scalar Multiplication**: $ \\| c\\mathbf{x} \\|_p = |c| \\cdot \\| \\mathbf{x} \\|_p $ for any scalar $ c $.\n",
    "- **Triangle Inequality**: $ \\| \\mathbf{x} + \\mathbf{y} \\|_p \\leq \\| \\mathbf{x} \\|_p + \\| \\mathbf{y} \\|_p $ for any vectors $ \\mathbf{x} $ and $ \\mathbf{y} $.\n",
    "\n",
    "These norms are extremely useful in various fields, including machine learning, where different norms can influence model behavior and performance, especially in regularization and optimization contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0899553",
   "metadata": {},
   "source": [
    "## Intuition and Physical World Analogies\n",
    "\n",
    "1. **Euclidean Norm (L2 Norm)**:\n",
    "   - **Definition**: $ \\| \\mathbf{x} \\|_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} $\n",
    "   - **Intuition**: This is the most familiar form of vector norm, representing the straight-line distance from the origin to the point in an n-dimensional space. It’s analogous to measuring the direct distance between two points using a ruler in physical space.\n",
    "   - **Physical Analogy**: In a 3D space, if you have a point defined by coordinates (3, 4, 0), the Euclidean norm gives the distance from this point to the origin (0, 0, 0), which corresponds to the hypotenuse of a right triangle formed by the points along the axes.\n",
    "\n",
    "2. **Manhattan Norm (L1 Norm)**:\n",
    "   - **Definition**: $ \\| \\mathbf{x} \\|_1 = |x_1| + |x_2| + \\cdots + |x_n| $\n",
    "   - **Intuition**: This norm measures the path between points if you can only travel along axis-aligned segments (like walking through the grid-like streets of Manhattan).\n",
    "   - **Physical Analogy**: Suppose you’re walking in a city with a strict grid system and you can only walk parallel to the streets. The distance you travel from one corner of a block to another, moving strictly along the streets, is given by the Manhattan norm.\n",
    "\n",
    "3. **Infinity Norm (Max Norm)**:\n",
    "   - **Definition**: $ \\| \\mathbf{x} \\|_\\infty = \\max(|x_1|, |x_2|, \\ldots, |x_n|) $\n",
    "   - **Intuition**: This norm is concerned with the maximum absolute value among the components of the vector.\n",
    "   - **Physical Analogy**: Imagine you are moving a set of boxes and you can only make one trip. The weight of the heaviest box you need to carry determines how tough the trip will be, regardless of how many lighter boxes there are. This heaviest box's weight is analogous to the infinity norm.\n",
    "\n",
    "### Importance of Vector Norms in Machine Learning\n",
    "\n",
    "Vector norms are crucial in machine learning for several reasons:\n",
    "\n",
    "1. **Regularization (L1 and L2 Norms)**:\n",
    "   - **L2 Regularization**: Often used in regression models (Ridge regression) to prevent overfitting by keeping the model weights small, effectively penalizing the sum of the squares of the weights.\n",
    "   - **L1 Regularization**: Used in Lasso regression to both prevent overfitting and perform automatic feature selection, as it tends to produce some coefficients that are exactly zero, thus excluding some features entirely.\n",
    "\n",
    "2. **Optimization**:\n",
    "   - Gradient descent algorithms, which are fundamental to training machine learning models, often involve norms to determine the size of steps taken in the parameter space toward the minimum of a loss function.\n",
    "\n",
    "3. **Distance Metrics**:\n",
    "   - In algorithms like K-nearest neighbors (KNN) or in clustering techniques, different norms can be used to calculate distances between data points, directly influencing the behavior of the algorithm.\n",
    "\n",
    "### Example: Calculating Different Norms in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96633d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Norm: [5.09901951 7.28010989 9.48683298]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a vector\n",
    "# x = np.array([3, -4, 5])\n",
    "\n",
    "# Euclidean norm (L2)\n",
    "l2_norm = np.linalg.norm(x, 2, axis=0)\n",
    "print(\"L2 Norm:\", l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db68d60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Norm: [ 6. 21.]\n"
     ]
    }
   ],
   "source": [
    "# Manhattan norm (L1)\n",
    "l1_norm = np.linalg.norm(x, 1)\n",
    "print(\"L1 Norm:\", l1_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31746728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinity Norm: 21.0\n"
     ]
    }
   ],
   "source": [
    "# Infinity norm (Max norm)\n",
    "inf_norm = np.linalg.norm(x, np.inf)\n",
    "print(\"Infinity Norm:\", inf_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae813b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [5, 7, 9]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [5,7,9]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd26092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Norm: 12.987681353947094\n"
     ]
    }
   ],
   "source": [
    "# Euclidean norm (L2)\n",
    "l2_norm = np.linalg.norm(x, 2)\n",
    "print(\"L2 Norm:\", l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe6805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Norm: [5.09901951 7.28010989 9.48683298]\n"
     ]
    }
   ],
   "source": [
    "l2_norm = np.linalg.norm(x, 2, axis=0)\n",
    "print(\"L2 Norm:\", l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868f9ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Norm: [ 3.74165739 12.4498996 ]\n"
     ]
    }
   ],
   "source": [
    "l2_norm = np.linalg.norm(x, 2, axis=1)\n",
    "print(\"L2 Norm:\", l2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae5f4e",
   "metadata": {},
   "source": [
    "# Applications of Vector Norms\n",
    "\n",
    "The $ L^1 $ norm (Manhattan norm), $ L^2 $ norm (Euclidean norm), and the $ L^{\\infty} $ norm (Max norm) are widely used in data science, machine learning, and various real-world applications. Each of these norms has distinct characteristics and applications based on their mathematical properties and the kind of robustness they offer to different problems.\n",
    "\n",
    "### $ L^1 $ Norm (Manhattan Norm)\n",
    "- **Feature Selection in Machine Learning**: The $ L^1 $ norm is particularly useful in sparse model learning where feature selection is crucial. In methods like Lasso (Least Absolute Shrinkage and Selection Operator) regression, the $ L^1 $ penalty promotes sparsity in the coefficients. By making some regression coefficients exactly zero, Lasso can effectively reduce the number of features, simplifying the model and helping in feature interpretation.\n",
    "- **Robustness to Outliers**: In statistics and data analysis, the $ L^1 $ norm can be more robust to outliers than the $ L^2 $ norm. When calculating distances between points or in optimization problems, the $ L^1 $ norm's linear growth with distance reduces the impact of large deviations or outliers compared to the quadratic growth of the $ L^2 $ norm.\n",
    "\n",
    "### $ L^2 $ Norm (Euclidean Norm)\n",
    "- **Regularization in Machine Learning**: The $ L^2 $ norm is commonly used for regularization in regression models (Ridge regression), where it helps to prevent overfitting by constraining the coefficients of the model. Unlike $ L^1 $, $ L^2 $ norm does not promote sparsity but instead smoothly penalizes the size of the coefficients.\n",
    "- **Data Normalization**: Before applying many machine learning algorithms, particularly those involving distance calculations like K-nearest neighbors (KNN) and support vector machines (SVM), data are often normalized using the $ L^2 $ norm. This normalization (often called feature scaling) ensures that each feature contributes equally to the distance computations, improving the performance of the algorithms.\n",
    "\n",
    "### $ L^{\\infty} $ Norm (Max Norm)\n",
    "- **Image Processing**: In image processing and computer graphics, the $ L^{\\infty} $ norm can be used to measure the maximum deviation between two images (e.g., an original and a compressed image). This is useful in scenarios where you need to ensure that no single pixel exceeds a certain difference threshold, which can be critical in quality control and medical imaging.\n",
    "- **Infinite Norm in Optimization Problems**: The $ L^{\\infty} $ norm is often used in optimization problems where the worst-case scenario is most critical. It ensures that the solution minimizes the maximum possible error, which is crucial in fields like operations research and network analysis.\n",
    "\n",
    "### Real-World Scenario Examples\n",
    "\n",
    "1. **Traffic Pattern Analysis (using $ L^1 $ norm)**: When analyzing traffic flow and patterns, the $ L^1 $ norm can help in creating models that are insensitive to extreme values caused by accidents or road closures, providing a more stable and representative analysis of typical traffic conditions.\n",
    "\n",
    "2. **Health Monitoring Systems (using $ L^2 $ norm)**: In health monitoring systems, the $ L^2 $ norm can be used to measure the overall deviation of a patient's health indicators from the norm. This can help in early detection of potential health issues based on continuous monitoring data.\n",
    "\n",
    "3. **Resource Allocation (using $ L^{\\infty} $ norm)**: In network resource allocation, especially in communication networks, the $ L^{\\infty} $ norm can be used to ensure that no single network link or node is overloaded beyond its capacity, which is crucial for maintaining network reliability and performance.\n",
    "\n",
    "These norms are fundamental tools in data science and applied mathematics, offering different ways to quantify magnitudes and manage trade-offs in sensitivity, robustness, and interpretability in various applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef089fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
